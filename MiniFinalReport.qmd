---
title: "Mini Final Report"
author: "Sam Lee, Jeffry Troll"
format: pdf
editor: visual
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{Times New Roman}
  - \usepackage{setspace}
  - \renewcommand{\normalsize}{\fontsize{12}{14.4}\selectfont}
  - \usepackage[margin=1in]{geometry}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggmap)
library(lubridate)
library(jsonlite)

#For zero-inflated Poisson
require(pscl)
require(boot)

#A package Sam made to report summary statistics
source("Sam/sum_stats.R")
```

# Introduction

Violent crime in Chicago remains a persistent challenge. The Chicago Police Department (CPD), particularly concerned about the situation in District 11, has partnered with Data Insight & Strategy Consultants (DISC) to explore innovative approaches. This project delves beyond traditional methods, incorporating human behavior, weather patterns, and even the lunar cycle, to analyze crime data and uncover potential predictors.

Our goal is to determine if and how these factors influence violent crime rates in Chicago. This knowledge will empower the CPD to proactively implement crime prevention strategies, such as public outreach and community events, during particularly vulnerable periods. By shedding light on the potential relationships between these factors and crime, we aim to make Chicago a safer place for all.

We start by analyzing the the number violent crimes that occur within District 11 over the past decade. We use the City of Chicago's definition (<https://www.chicago.gov/city/en/sites/vrd/home/violence-victimization.html>) to define violent crimes. To elicit the effect that weather has on crimes, we also divide out the crime by hour. We use historical weather data imported by a [weather API](https://open-meteo.com/) to map hourly weather data to the hour the crime happened.

To analyze the predictive performance of these covariates, we employ several models. For more interpretation on the coefficients, we first train a generalized linear model adjusted with time-series components. For more predictive power, we employ more flexible models, including random forests, KNN, and clusters analysis.

We will first introduce the data used for this analysis, followed by some EDA we used to explore the data, the models we estimated, the results of our analysis, and then conclude.

# Data Acquisition and Preprocessing

## Primary Data Sources:

To conduct a comprehensive analysis of violent crime in Chicago's District 11, DISC acquired data from the following reputable sources:

-   Chicago Crime Data: Detailed records of crimes reported in District 11 since 2001 were obtained from the official City of Chicago Data Portal <https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2>. We opted for this source to ensure data accuracy and consistency with official crime reporting procedures. Additionally, a valuable Kaggle resource <https://www.kaggle.com/datasets/chicago/chicago-crime> provided insights into the data structure and potential applications.

-   Weather Data: Daily weather data for Chicago from 2010 to 2024, encompassing temperature, precipitation, wind speed, and other relevant meteorological variables, was originally retrieved from Visual Crossing (https://www.visualcrossing.com/). However, we asserted that to model the demand for violent crime more granually, as crimes were recorded by the hour; as such, we imported weather data from an historical weather API, <https://open-meteo.com/>.

## Secondary Data Sources:

-   Holiday Data: This data was obtained via ChatGPT for initial exploration, followed by cross-referencing with reliable online resources like national holiday calendars and local Chicago event listings.

-   Full Moon Data: Dates of full moons since 2005 were sourced from Full Moon Info <https://www.fullmoonology.com/full-moon-calendar-2015/>, a website recognized for its comprehensive moon phase calendar. This data serves as a starting point for exploring potential lunar influences on crime rates.

## Data Harmonization and Feature Engineering:

To ensure a consistent temporal scope across all data sets, a decision was made to utilize data from 2010 to 2024. While this approach sacrifices some crime data from the early 2000s, it allows for a more robust and comparable analysis with the available weather and holiday information.

Furthermore, monthly unemployment data for Chicago from 2010 to 2024 was incorporated as a socioeconomic indicator through using data from the Bureau of Labor Statistics.

Here's how we wrangled all the data files in R:

```{r data-wrangle, message=F, echo=TRUE}
tryCatch({
  maps.api.key = read_file("Sam/maps_api.txt")
})

crimes <- read_csv("Sam/Data/Crimes.csv")
crimes$DateTime <- mdy_hms(crimes$Date)
crimes$Date <- as.Date(crimes$DateTime)
crimes$hour <- hour(crimes$DateTime)

moons <- read_csv("Sam/Data/full_moon.csv") %>%
  mutate(Date = dmy(FullMoonDates))

holidays <- read_csv("Sam/Data/holidays.csv") %>%
  mutate(Date = ymd(Date))

#Weather Data from API
weather.data <- fromJSON("Sam/Data/weather.json")["hourly"] %>% 
  as.data.frame()
weather.data$hourly.time = weather.data$hourly.time %>% 
  str_replace("T", " ")
weather.data$hourly.time = ymd_hm(weather.data$hourly.time)
weather.data <- weather.data %>%
  mutate(
    Date = as.Date(hourly.time),
    hour = hour(hourly.time) %>%
      as.numeric()
  )
weather.covariates <- colnames(weather.data)[2:(ncol(weather.data)-2)]

cutoff.date = max(
  c(min(crimes$Date), min(holidays$Date), min(weather.data$Date))
)
upper.cutoff = min(
  c(max(crimes$Date), max(holidays$Date), max(weather.data$Date))
)

crimes <- weather.data %>% left_join(crimes) %>%
  mutate(
    DateTime = hourly.time,
    Year = year(DateTime)
  )

week.days <- c("Mon", "Tues", "Wed", "Thurs", "Fri", "Sat", "Sun")

crimes.cleaned <- crimes %>%
  left_join(holidays) %>%
  left_join(moons) %>%
  mutate(
    Holiday = ifelse(is.na(Holiday), "", Holiday),
    DayofWeek = week.days[wday(Date, week_start=1)],
    FullMoon = ifelse(is.na(FullMoonDates),0,1)
  ) %>% filter(
    Date >= cutoff.date & Date <= upper.cutoff
  )
  
factors = c("DateTime", "Date", "Primary Type", "Location Description",
            "Arrest", "Domestic", "Community Area", "Year", "Latitude",
            "Longitude", "FullMoon", "DayofWeek", "Holiday", "hour",
            weather.covariates)

crimes.cleaned <- crimes.cleaned[,factors]

#In the FBI's Uniform Crime Reporting (UCR) Program, violent crime is composed 
#of four offenses: murder and nonnegligent manslaughter, forcible rape, 
#robbery, and aggravated assault.

#https://www.chicago.gov/city/en/sites/vrd/home/violence-victimization.html
allcrimes = crimes.cleaned$`Primary Type` %>% 
  unique() %>% 
  sort()
violence.key <- c(0,1,1,1,0,1,0,1,0,0,0,1,1,0,1,1,0,0,0,0,
                  0,0,0,0,0,0,0,0,0,0,1,0,0,0,0)
mapping <- setNames(seq_along(unique(allcrimes)), unique(allcrimes))
crimes.cleaned$Violent <- violence.key[
  unname(mapping[crimes.cleaned$`Primary Type`])
]

crimes.cleaned <- crimes.cleaned %>%
  mutate(
    #These are the hours where there weren't any violent crimes 
    Violent = ifelse(is.na(Violent), 0, Violent)
  )

crimes.cleaned <- crimes.cleaned %>% group_by(Violent, Date, hour) %>%
  mutate(
    NumViolentCrimes = Violent*n()
  ) %>% ungroup()

#Merge in unemployment data from BLS
unem <- read_csv("Sam/Data/chicago-unemployment.csv") %>%
  dplyr::select(Year, Label, Value) %>%
  mutate(
    Date = ym(Label),
    Month = month.name[month(Date)]
  ) %>% dplyr::select(-Label) %>% setNames(c(
    "Year", "Unemployment", "Date", "Month"
  )) %>% arrange(Date)

crimes.cleaned$Month <- month.name[month(crimes.cleaned$Date)]

#Add in monthly unemployment
crimes.cleaned <- crimes.cleaned %>%
  left_join(
    unem %>% dplyr::select(-Date), by=join_by(Year, Month)
  )
```

Note that we define `NumViolentCrimes` as the number of violent crimes that happen within a given hour. This is one of our response variables.

# EDA

**Summary Statistics**

```{r summary-stats, echo=F}
summary.factors = c("Community Area", "Latitude", "Longitude",
                    weather.covariates, "Violent", "NumViolentCrimes",
                    "Unemployment")

sum.stats.table(crimes.cleaned[summary.factors[1:3]]) %>%
  knitr::kable(caption="Summary Statistics of Location Variables")


sum.stats.rownames <- c("Temperature", "Humidity", "Apparent Temperature",
                        "Rain", "Snowfall", "Snow Depth", "Cloud Cover",
                        "Wind Speed", "Wind Gusts", "Is Day Time",
                        "Shortwave Radiation", "Direct Radiation",
                        "Is Violent Crime", "Hourly Violent Crimes",
                        "Unemployment")
sum.stats.table(crimes.cleaned[summary.factors[4:length(summary.factors)]],
                sum.stats.rownames) %>%
  knitr::kable(caption="Summary Statistics of Numerical Variables")
```

Note that summary statistics come from hourly records of crimes. The NA values on the location summary statistics are the hours where no crimes were observed.

**Is there any spatial correlation between where crimes are occurring?**

```{r spatial-map, fig.width=12, fig.height=7, message=F, warning=F, echo=F}
register_google(key=maps.api.key)
chicago_map <- get_map(location = c(lon=-87.72, lat = 41.881832),
                       zoom = 13)

crimes.map <- crimes.cleaned %>% filter(Year == 2023) %>%
  mutate(
    Violent = as.factor(Violent)
  )
ggmap(chicago_map) +
  geom_point(data = crimes.map,
             aes(x = Longitude, y = Latitude, color=Violent),
             alpha=0.2) +
  labs(title = "Crimes in District 11 During 2023")
```

Based on the map above, since District 11 is such a small area, and since we don't have other data on the where crime is occurring in other districts, from this preliminary visual assessment, it doesn't look like there are any strong hotspots for violent crime throughout the year.

**How do the number of violent crimes generally fluctuate throughout the year?**

```{r between-within-trends, echo=F, fig.width=14, fig.height=7}
crimes.cleaned %>%
  filter(Year %in% c(2010, 2015, 2020, 2023)) %>%
  mutate(
    modified.date = paste0("2024-", sprintf("%02s", Month), "-", sprintf("%02s", day(Date))) %>% ymd(),
    Year = as.factor(Year)
  ) %>%
  group_by(Date) %>%
  mutate(
    n_violent = sum(Violent),
  ) %>%
  ggplot() +
  geom_line(aes(x = modified.date, y = n_violent, group = Year, color = Year)) +
  labs(
    x = "Date",
    y = "# of Daily Violent Crimes",
    title = "Violent Crime Trends Between and Within Years"
  ) +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")
```

While it's hard to read in between the lines (literally), this graph shows that crime generally peaks during the summer months, while at the same time, crime has generally been decreasing over the pass decade.

**What does the distribution of hourly violent crimes look like?**

```{r violent-crimes-hist, echo=F, fig.width=14, fig.height=7}
crimes.cleaned %>% ggplot()+
  geom_histogram(aes(x=NumViolentCrimes), binwidth=1, fill="darkblue", alpha=0.75)+
  scale_x_continuous(breaks=0:8)+
  labs(
    x="Number of Violent Crimes per Hour",
    y="Frequency"
  )+
  theme_minimal()
```

This appears that the distribution of the hourly violent crimes is Poisson-distributed. Specifically, since there's a (greater than expected) concentration of counts at 0, we first thought about modeling hourly violent number of crimes via a zero-inflated Poisson model (ZIP).

**How does unemployment fluctuate throughout the observed decade in Chicago?**

```{r unemployment, echo=F, message=F, warning=F, fig.width=14, fig.height=7}
unem %>% ggplot(aes(x=Date, y=Unemployment))+
  geom_point()+
  geom_line()+
  labs(
    title="Monthly Unemployment Rates from the BLS"
  )+
  theme_minimal()+
  geom_smooth(method="gam")
```

Unemployment seems to generally decrease throughout the decade (accounting for the seasonal fluctuations). There is a large spike around the COVID months. This could account for the spike we see in crime in the summer months in 2020. We include unemployment as a factor in our models.

**How does the number of violent crime fluctuate throughout the hour of day and day of the week?**

![](crimes-day-hour.png)

It appears that the early hours of the morning are only prone to more violent crime during the weekend, while violent crimes typically occur in the afternoon on weekdays. We want to account for this in our models.

# Data Analysis, Model Building, and Interpretation

## Generalized Linear Models

We first attempted to understand how the different coefficients were affecting our response variable, the number of violent crimes per hour. We employed several generalized linear models:

1) Poisson Regression
2) Negative Binomial Regression
3) Lasso Regression
4) Zero-Inflated Poisson Regression

The main autoregressive linear model proceeds as follows:

For a given hour ($h$), day ($d$), month ($m$), and year ($y$), we model the number of number of hourly violent crimes ($Y_{hdmy}$) as follows:

$$
(1a) \quad Y_{hdmy} = \beta_0+\beta_1\text{FullMoon}_{dmy}+\beta_2\text{Unemployment}_{my}+X_{hdmy}'\gamma+\text{Weather}_{hdmy}'\Omega
$$
$$
+\sum_{i=1}^{23}\eta_i\mathbb{1}(\text{Hour}_h=i)+\sum_{i=Mon}^{Sat}\delta_i\mathbb{1}(\text{Day}_d=i)+\sum_{i=Jan}^{Nov}\mu_i\mathbb{1}(\text{Month}_m=i)
$$
$$
+\sum_{i=1}^{23}\sum_{j=Mon}^{Sat}\xi_{6(i-1)+j}\mathbb{1}(\text{Hour}_h=i)\mathbb{1}(\text{Day}_d=j)+\sum_{i=1}^{24}\psi_iY_{(h-i)dmy}+\epsilon_{hdmy}
$$
$$
(1b) \quad \mathbb{E}[\epsilon_{hdmy}|\mathbb{X}_{hdmy}]=0 \quad \forall h,d,m,y
$$

Where $\mathbb{X}_{hdmy}$ is the complete covariate matrix (including fixed effects and autoregressive lags). The $X$ matrix includes covariates such as holidays and other miscellaneous interactions that vary from model to model.

However, to get better predictive power, as discussed previously, we decided to model the functional of $Y_{hdmy}$ more appropriately through a countwise regression. We found that a Zero-Inflated Poisson (ZIP) regression ultimately performed the best out of the four genderalized linear models we performed above. Hence, we generalized (1) to model both the mean ($\mathbb{E}[Y_{hdmy}|\mathbb{X}_{hdmy}]$) of the hourly number of violent crimes and the probability that no violent crimes would be committed during that specific hour (this is structurally equivalent to a ZIP regression model).

Hence, we generalized (1) into the following ZIP regression model:

$$
(2a) \quad Y_{hdmy} \sim ZIP(\lambda_{hdmy}, \pi)
$$
$$
(2b) \quad \lambda_{hdmy} = exp\{\beta_0+\beta_1\text{FullMoon}_{dmy}+\beta_2\text{Unemployment}_{my}+X_{hdmy}'\gamma+\text{Weather}_{hdmy}'\Omega
$$
$$
+\sum_{i=1}^{23}\eta_i\mathbb{1}(\text{Hour}_h=i)+\sum_{i=Mon}^{Sat}\delta_i\mathbb{1}(\text{Day}_d=i)+\sum_{i=Jan}^{Nov}\mu_i\mathbb{1}(\text{Month}_m=i)
$$
$$
+\sum_{i=1}^{23}\sum_{j=Mon}^{Sat}\xi_{6(i-1)+j}\mathbb{1}(\text{Hour}_h=i)\mathbb{1}(\text{Day}_d=j)+\sum_{i=1}^{24}\psi_iY_{(h-i)dmy}\}
$$
Where $\pi$ is the probability that $0$ crimes occur during a given hour.

Additionally, we used a penalized lasso poisson regression regression to determine which covariates to include in $\text{Weather}_{hdmy}$ and $X_{hdmy}$.

Lasso Poisson Regression Models[^1]:

[^1]: Perhaps a bit of notational inconsistency here, but $t$ represents an individual observation in the data set here, which is equivalently a single observed hour ($h$) in a given day.

$$
(3) \quad \underset{b_0, b}{argmin} \sum_{t=1}^n\left( y_{tdmy} - exp\{ b_0 + \text{Weather}_{tdmy}'b\}\right)^2+\alpha([1,..,1]'b)
$$
$$
(4) \quad \underset{b_0, b}{argmin} \sum_{t=1}^n\left( y_{tdmy} - exp\{ b_0 + X_{tdmy}'b\}\right)^2+\lambda([1,..,1]'b)
$$

Using K-fold CV ($k=10$), we used $\alpha=0.0003305944$, and $\lambda=0.0009574824$. With these hyperparameters, we used the lasso model to find the most important factors to yield a more parsimonious model. Based on the results of the lasso models, we ended omitting ``Apparent Temperature``, ``Wind Speed``, and ``Direct Solar Radiation`` out of the weather covariates. In the $X$ covariate matrix, we also omitted the ``Columbus Day`` dummy variable (we have included indicators for every federal holiday).

Having these variables excluded from model (2), we estimated the parameters using maximum likelihood estimation (MLE).

The ZIP model achieved a MSE of 0.76. Here are the summary plots of the estimated coefficients[^2]:

[^2]: The numerical coefficients were all standardized for relative interpretability.

```{=latex}
\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{Sam/catepillar-plot-most-important.png}
\caption{Coefficients with Largest Magnitudes}
\label{fig:sub1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{Sam/catepillar-interest.png}
\caption{Coefficients Omitting Time Fixed Effects}
\label{fig:sub2}
\end{subfigure}
\caption{Estimated Parameter Coefficients of ZIP Model}
\label{fig:whole}
\end{figure}
```

As fortold by our EDA, temperature is strongly correlated with the propensity to commit violent crimes in Chicago. Interestingly, the most significant (non-fixed effect) is `New Year's Day`. This is the holiday where the most violent crimes are committed.

While visually showing how well the model predicts on all hours may be hard to show in a single plot, we selected a random sample (two weeks) to overlay both the predicted mean of violent crimes and a bootstrap predicted sample of the actual number of violent crimes (shaded in green) to simulate a 95% confidence interval.

```{=latex}
\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{Sam/eda.png}
\caption{The hourly distribution of violent crimes is (zero-inflated) Poisson-distributed}
\label{fig:ViolentCrimes}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
\includegraphics[width=\textwidth]{Sam/PredictiveAnalytics.png}
\caption{Random sample overlaid with predicted mean and predicted bootstrapped 95\% confidence interval}
\label{fig:Prediction}
\end{subfigure}
\caption{Visual predictive performance of ZIP model}
\label{fig:whole}
\end{figure}
```

This model explains the effects of the factors included in the model quite well, although the prediction is quite noisy. To achieve better prediction of violent crimes, we explore more flexible machine learning models.

# Predictive model

Implementation of at least four standard supervised machine learning models We tried a variety of model to get try to get better predictions and compare those results. Indeed, we fit a Random Forest model, Decision Tree, Gradient boosting and KNN model and already mentioned Poisson model.

KNN
```{python,echo=TRUE, eval=FALSE}
knn_reg = KNeighborsRegressor(n_neighbors=20)
knn_reg.fit(X_train_scaled, y_train)
```

Mean Absolute Error: 3.495668789808917
Mean Squared Error: 18.586433121019112
Root Mean Squared Error: 4.311198571281438
R-squared: 0.018065556289428297

![Image Description](output6.png)
Random Forest
```{python,echo=TRUE, eval=FALSE}
knn_reg = KNeighborsRegressor(n_neighbors=20)
knn_reg.fit(X_train_scaled, y_train)
```

Mean Absolute Error: 3.254050955414013
Mean Squared Error: 16.357082547770705
Root Mean Squared Error: 4.044389020330599
R-squared: 0.1358437281809961

![Image Description](output7.png)
Gradient boosting

Mean Absolute Error: 3.2315556864642456
Mean Squared Error: 16.022146503614472
Root Mean Squared Error: 4.002767355669634
R-squared: 0.15353863693813885

# Significant or innovative feature engineering

Since we noticed that that time was an important factor, we considered to add generate more variables that can handle the seasonality presented in the data.

```{python,echo=TRUE, eval=FALSE}
data['Month_Sin'] = np.sin(2 * np.pi * data['Month']/12)
data['Month_Cos'] = np.cos(2 * np.pi * data['Month']/12)
data['Day_Sin'] = np.sin(2 * np.pi * data['Day']/data['Date'].dt.days_in_month)
data['Day_Cos'] = np.cos(2 * np.pi * data['Day']/data['Date'].dt.days_in_month)
```

This feature engineering lies in capturing temporal patterns in a way that is more suitable for machine learning models, like this Random Forest. By encoding months and days as cyclical features, the model can understand their cyclic nature and learn patterns effectively, such as monthly or daily seasonality which is present in the crime data because of weather and another factors.

# Exploration of variable importance using SHAP

As we mentioned before, we used a Decision Tree to understand better the factors that impact the count of violent crime.

```{python, echo=TRUE, eval=FALSE}
rf_model = RandomForestRegressor(n_estimators=20, random_state=42)

# Fit the model
rf_model.fit(X, y)

# Get feature importances
feature_importances = rf_model.feature_importances_

# Map feature names to their respective importances
feature_importance_map = dict(zip(X.columns, feature_importances))

# Sort the features by importance
sorted_features = sorted(feature_importance_map.items(), key=lambda x: x[1], reverse=True)

# Extract top 25 features
top_features = sorted_features[:25]
```

![Image Description](output.png) We kind of expecting that that temperature will be an important factor, so we will check with SHAP as well to understand the impact of features on model predictions.

```{python, echo=TRUE, eval=FALSE}
#Fitting the model
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

#Getting SHAP values and plotting them
explainer = shap.Explainer(rf, X_train)
shap_values = explainer.shap_values(X_test[:100])
shap.summary_plot(shap_values, features=X_test[:100], feature_names=X_train.columns)
```

![Image Description](output2.png) In summary, thanks to the SHAP values analysis, it seems that the model seems to be influenced significantly by perceived temperature (feelslike) and actual temperature measures, with time-related features also playing a key role. Lower on the importance scale are some other weather-related features and atmospheric conditions.

# Cluster analysis OR anomaly detection 
Also we wanted to apply cluster analysis to understand patterns or groups in how these factors correlate with crime rates

```{python, echo=TRUE, eval=FALSE}
#Starting clustering analysis
kmeans = KMeans(n_clusters=3, n_init='auto')
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

hc = AgglomerativeClustering(distance_threshold=None, n_clusters=3)
hc.fit(X)
linkage_methods = ['single', 'complete', 'average', 'ward']

# Plotting
plt.figure(figsize=(12, 8))
for i, method in enumerate(linkage_methods, 1):
    plt.subplot(2, 2, i)
    Z = linkage(X, method=method)
    dendrogram(Z)
    plt.title(f'Dendrogram ({method.capitalize()} Linkage)')
    plt.xlabel('Samples')
    plt.ylabel('Distance')

plt.tight_layout()
plt.show()
```

![Image Description](output3.png) It seems that they're 3 main clusters in the data. This is confirm with the "elbow method" technique, since around 3 or 4 it seems to be the place where the rate of decrease in WCSS slows down significantly.

```{python, echo=TRUE, eval=FALSE}
# Calculating WCSS by K number of clusters for plot
kmeans_per_k = [KMeans(n_clusters=k, n_init='auto', random_state=42).fit(X)
                for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]


#Plotting WCSS by number of clusters
plt.figure(figsize=(10,4))
plt.plot(np.arange(len(inertias))+1,inertias,marker="o")
plt.xlabel('Number of Clusters, K')
plt.ylabel('WCSS')
```

![Image Description](output4.png) After defining the number of clusters, we can start analyzing the major factors with these 3 clusters.

```{python, echo=TRUE, eval=FALSE}
#Selecting factors to see
plot_df = pd.DataFrame(X[['feelslike', 'Year', 'YrMon', 'Date', 'tempmax', 'IsWeekend', 'feelslikemax', 'temp', 'DayOfYear', 'WeekOfYear']])

#Plotting
plot_df['clusters'] = y_kmeansf Clusters, K')
plt.ylabel('WCSS')
sns.pairplot(plot_df, hue='clusters', palette='Dark2')
```

![Image Description](output5.png) From the clustering analysis, we can learn that the feature relationships, distribution, and seasonality.
