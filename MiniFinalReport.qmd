---
title: "Mini Final Report"
author: "Sam Lee, Jeffry Troll"
format: pdf
editor: visual
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{Times New Roman}
  - \usepackage{setspace}
  - \renewcommand{\normalsize}{\fontsize{12}{14.4}\selectfont}
  - \usepackage[margin=1in]{geometry}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggmap)
library(lubridate)
library(jsonlite)

#For zero-inflated Poisson
require(pscl)
require(boot)

#A package Sam made to report summary statistics
source("Sam/sum_stats.R")
```

# Introduction

Violent crime in Chicago remains a persistent challenge. The Chicago Police Department (CPD), particularly concerned about the situation in District 11, has partnered with Data Insight & Strategy Consultants (DISC) to explore innovative approaches. This project delves beyond traditional methods, incorporating human behavior, weather patterns, and even the lunar cycle, to analyze crime data and uncover potential predictors.

Our goal is to determine if and how these factors influence violent crime rates in Chicago. This knowledge will empower the CPD to proactively implement crime prevention strategies, such as public outreach and community events, during particularly vulnerable periods. By shedding light on the potential relationships between these factors and crime, we aim to make Chicago a safer place for all.

We start by analyzing the the number violent crimes that occur within District 11 over the past decade. We use the City of Chicago's definition (<https://www.chicago.gov/city/en/sites/vrd/home/violence-victimization.html>) to define violent crimes. To elicit the effect that weather has on crimes, we also divide out the crime by hour. We use historical weather data imported by a [weather API](https://open-meteo.com/) to map hourly weather data to the hour the crime happened.

To analyze the predictive performance of these covariates, we employ several models. For more interpretation on the coefficients, we first train a generalized linear model adjusted with time-series components. For more predictive power, we employ more flexible models, including random forests, KNN, and clusters analysis.

We will first introduce the data used for this analysis, followed by some EDA we used to explore the data, the models we estimated, the results of our analysis, and then conclude.

# Data Acquisition and Preprocessing

## Primary Data Sources:

To conduct a comprehensive analysis of violent crime in Chicago's District 11, DISC acquired data from the following reputable sources:

-   Chicago Crime Data: Detailed records of crimes reported in District 11 since 2001 were obtained from the official City of Chicago Data Portal <https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2>. We opted for this source to ensure data accuracy and consistency with official crime reporting procedures. Additionally, a valuable Kaggle resource <https://www.kaggle.com/datasets/chicago/chicago-crime> provided insights into the data structure and potential applications.

-   Weather Data: Daily weather data for Chicago from 2010 to 2024, encompassing temperature, precipitation, wind speed, and other relevant meteorological variables, was originally retrieved from Visual Crossing (https://www.visualcrossing.com/). However, we asserted that to model the demand for violent crime more granually, as crimes were recorded by the hour; as such, we imported weather data from an historical weather API, <https://open-meteo.com/>.

## Secondary Data Sources:

-   Holiday Data: This data was obtained via ChatGPT for initial exploration, followed by cross-referencing with reliable online resources like national holiday calendars and local Chicago event listings.

-   Full Moon Data: Dates of full moons since 2005 were sourced from Full Moon Info <https://www.fullmoonology.com/full-moon-calendar-2015/>, a website recognized for its comprehensive moon phase calendar. This data serves as a starting point for exploring potential lunar influences on crime rates.

## Data Harmonization and Feature Engineering:

To ensure a consistent temporal scope across all data sets, a decision was made to utilize data from 2010 to 2024. While this approach sacrifices some crime data from the early 2000s, it allows for a more robust and comparable analysis with the available weather and holiday information.

Furthermore, monthly unemployment data for Chicago from 2010 to 2024 was incorporated as a socioeconomic indicator through using data from the Bureau of Labor Statistics.

Here's how we wrangled all the data files in R:

```{r data-wrangle, message=F, echo=TRUE}
tryCatch({
  maps.api.key = read_file("Sam/maps_api.txt")
})

crimes <- read_csv("Sam/Data/Crimes.csv")
crimes$DateTime <- mdy_hms(crimes$Date)
crimes$Date <- as.Date(crimes$DateTime)
crimes$hour <- hour(crimes$DateTime)

moons <- read_csv("Sam/Data/full_moon.csv") %>%
  mutate(Date = dmy(FullMoonDates))

holidays <- read_csv("Sam/Data/holidays.csv") %>%
  mutate(Date = ymd(Date))

#Weather Data from API
weather.data <- fromJSON("Sam/Data/weather.json")["hourly"] %>% 
  as.data.frame()
weather.data$hourly.time = weather.data$hourly.time %>% 
  str_replace("T", " ")
weather.data$hourly.time = ymd_hm(weather.data$hourly.time)
weather.data <- weather.data %>%
  mutate(
    Date = as.Date(hourly.time),
    hour = hour(hourly.time) %>%
      as.numeric()
  )
weather.covariates <- colnames(weather.data)[2:(ncol(weather.data)-2)]

cutoff.date = max(
  c(min(crimes$Date), min(holidays$Date), min(weather.data$Date))
)
upper.cutoff = min(
  c(max(crimes$Date), max(holidays$Date), max(weather.data$Date))
)

crimes <- weather.data %>% left_join(crimes) %>%
  mutate(
    DateTime = hourly.time,
    Year = year(DateTime)
  )

week.days <- c("Mon", "Tues", "Wed", "Thurs", "Fri", "Sat", "Sun")

crimes.cleaned <- crimes %>%
  left_join(holidays) %>%
  left_join(moons) %>%
  mutate(
    Holiday = ifelse(is.na(Holiday), "", Holiday),
    DayofWeek = week.days[wday(Date, week_start=1)],
    FullMoon = ifelse(is.na(FullMoonDates),0,1)
  ) %>% filter(
    Date >= cutoff.date & Date <= upper.cutoff
  )
  
factors = c("DateTime", "Date", "Primary Type", "Location Description",
            "Arrest", "Domestic", "Community Area", "Year", "Latitude",
            "Longitude", "FullMoon", "DayofWeek", "Holiday", "hour",
            weather.covariates)

crimes.cleaned <- crimes.cleaned[,factors]

#In the FBI's Uniform Crime Reporting (UCR) Program, violent crime is composed 
#of four offenses: murder and nonnegligent manslaughter, forcible rape, 
#robbery, and aggravated assault.

#https://www.chicago.gov/city/en/sites/vrd/home/violence-victimization.html
allcrimes = crimes.cleaned$`Primary Type` %>% 
  unique() %>% 
  sort()
violence.key <- c(0,1,1,1,0,1,0,1,0,0,0,1,1,0,1,1,0,0,0,0,
                  0,0,0,0,0,0,0,0,0,0,1,0,0,0,0)
mapping <- setNames(seq_along(unique(allcrimes)), unique(allcrimes))
crimes.cleaned$Violent <- violence.key[
  unname(mapping[crimes.cleaned$`Primary Type`])
]

crimes.cleaned <- crimes.cleaned %>%
  mutate(
    #These are the hours where there weren't any violent crimes 
    Violent = ifelse(is.na(Violent), 0, Violent)
  )

crimes.cleaned <- crimes.cleaned %>% group_by(Violent, Date, hour) %>%
  mutate(
    NumViolentCrimes = Violent*n()
  ) %>% ungroup()

#Merge in unemployment data from BLS
unem <- read_csv("Sam/Data/chicago-unemployment.csv") %>%
  dplyr::select(Year, Label, Value) %>%
  mutate(
    Date = ym(Label),
    Month = month.name[month(Date)]
  ) %>% dplyr::select(-Label) %>% setNames(c(
    "Year", "Unemployment", "Date", "Month"
  )) %>% arrange(Date)

crimes.cleaned$Month <- month.name[month(crimes.cleaned$Date)]

#Add in monthly unemployment
crimes.cleaned <- crimes.cleaned %>%
  left_join(
    unem %>% dplyr::select(-Date), by=join_by(Year, Month)
  )
```

Note that we define `NumViolentCrimes` as the number of violent crimes that happen within a given hour. This is one of our response variables.

# EDA

**Summary Statistics**

```{r summary-stats, echo=F}
summary.factors = c("Community Area", "Latitude", "Longitude",
                    weather.covariates, "Violent", "NumViolentCrimes",
                    "Unemployment")

sum.stats.table(crimes.cleaned[summary.factors[1:3]]) %>%
  knitr::kable(caption="Summary Statistics of Location Variables")


sum.stats.rownames <- c("Temperature", "Humidity", "Apparent Temperature",
                        "Rain", "Snowfall", "Snow Depth", "Cloud Cover",
                        "Wind Speed", "Wind Gusts", "Is Day Time",
                        "Shortwave Radiation", "Direct Radiation",
                        "Is Violent Crime", "Hourly Violent Crimes",
                        "Unemployment")
sum.stats.table(crimes.cleaned[summary.factors[4:length(summary.factors)]],
                sum.stats.rownames) %>%
  knitr::kable(caption="Summary Statistics of Numerical Variables")
```

Note that summary statistics come from hourly records of crimes. The NA values on the location summary statistics are the hours where no crimes were observed.

**Is there any spatial correlation between where crimes are occurring?**

```{r spatial-map, fig.width=12, fig.height=7, message=F, warning=F, echo=F}
register_google(key=maps.api.key)
chicago_map <- get_map(location = c(lon=-87.72, lat = 41.881832),
                       zoom = 13)

crimes.map <- crimes.cleaned %>% filter(Year == 2023) %>%
  mutate(
    Violent = as.factor(Violent)
  )
ggmap(chicago_map) +
  geom_point(data = crimes.map,
             aes(x = Longitude, y = Latitude, color=Violent),
             alpha=0.2) +
  labs(title = "Crimes in District 11 During 2023")
```

Based on the map above, since District 11 is such a small area, and since we don't have other data on the where crime is occurring in other districts, from this preliminary visual assessment, it doesn't look like there are any strong hotspots for violent crime throughout the year.

**How do the number of violent crimes generally fluctuate throughout the year?**

```{r between-within-trends, echo=F, fig.width=14, fig.height=7}
crimes.cleaned %>%
  filter(Year %in% c(2010, 2015, 2020, 2023)) %>%
  mutate(
    modified.date = paste0("2024-", sprintf("%02s", Month), "-", sprintf("%02s", day(Date))) %>% ymd(),
    Year = as.factor(Year)
  ) %>%
  group_by(Date) %>%
  mutate(
    n_violent = sum(Violent),
  ) %>%
  ggplot() +
  geom_line(aes(x = modified.date, y = n_violent, group = Year, color = Year)) +
  labs(
    x = "Date",
    y = "# of Daily Violent Crimes",
    title = "Violent Crime Trends Between and Within Years"
  ) +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%b")
```

While it's hard to read in between the lines (literally), this graph shows that crime generally peaks during the summer months, while at the same time, crime has generally been decreasing over the pass decade.

**What does the distribution of hourly violent crimes look like?**

```{r violent-crimes-hist, echo=F, fig.width=14, fig.height=7}
crimes.cleaned %>% ggplot()+
  geom_histogram(aes(x=NumViolentCrimes), binwidth=1, fill="darkblue", alpha=0.75)+
  scale_x_continuous(breaks=0:8)+
  labs(
    x="Number of Violent Crimes per Hour",
    y="Frequency"
  )+
  theme_minimal()
```

This appears that the distribution of the hourly violent crimes is Poisson-distributed. Specifically, since there's a (greater than expected) concentration of counts at 0, we first thought about modeling hourly violent number of crimes via a zero-inflated Poisson model (ZIP).

**How does unemployment fluctuate throughout the observed decade in Chicago?**

```{r unemployment, echo=F, message=F, warning=F, fig.width=14, fig.height=7}
unem %>% ggplot(aes(x=Date, y=Unemployment))+
  geom_point()+
  geom_line()+
  labs(
    title="Monthly Unemployment Rates from the BLS"
  )+
  theme_minimal()+
  geom_smooth(method="gam")
```

Unemployment seems to generally decrease throughout the decade (accounting for the seasonal fluctuations). There is a large spike around the COVID months. This could account for the spike we see in crime in the summer months in 2020. We include unemployment as a factor in our models.

**How does the number of violent crime fluctuate throughout the hour of day and day of the week?**

![](crimes-day-hour.png)

It appears that the early hours of the morning are only prone to more violent crime during the weekend, while violent crimes typically occur in the afternoon on weekdays. We want to account for this in our models.

# Data Analysis, Model Building, and Interpretation

## Generalized Linear Models

We first attempted to understand how the different coefficients were affecting our response variable, the number of violent crimes per hour. We employed several generalized linear models:

1) Poisson Regression
2) Negative Binomial Regression
3) Lasso Regression
4) Zero-Inflated Poisson Regression

The main linear model proceeds as follows:



Main Model

$y \sim MVN(\textbf{X}\mathbf{\beta}, \sigma^2\textbf{R}({\phi},{\omega}))$, $\textbf{y} = \textbf{X}\mathbf{\beta} + \epsilon$, where $\epsilon \sim N(0,\sigma^2R({\phi},{\omega}))$

$\textbf{y} = \begin{bmatrix} WHC_{1}\\ WHC_{2}\\ \vdots \\ WHC_{528} \end{bmatrix}$,

$\textbf{X} = \begin{bmatrix} 1 & x_{11} & x_{12} \\ 1 & x_{21} & x_{22} \\ \vdots & \vdots & \vdots \\ 1 & x_{5281} & x_{5282} \end{bmatrix}$, $\boldsymbol{\beta}= \begin{bmatrix} \beta_{0} \\ \beta_{Yield} \\ \beta_{EC} \end{bmatrix}$,

$\textbf{R} = \begin{bmatrix} 1 & \rho(s_1,s_2) & \rho(s_1,s_3) & \dots & \rho(s_1,s_{528}) \\ \rho(s_2,s_1) & 1 & \rho(s_2,s_3) & \dots & \rho(s_2,s_{528}) \\ \vdots & \vdots & \ddots & 1 & \rho(s_{527},s_{528}) \\ \rho(s_{528},s_1) & \rho(s_{528},s_2) & \rho(s_{528},s_3) & \dots & 1\end{bmatrix}$, where $\rho(s_i, s_j) = exp\{-\frac{||s_i-s_j||}{\phi}\}$.

Within our model, we have 5 parameters.

-   $\beta_0$ represents the intercept, which is the expected value when all other coefficients within our model are zero.\

-   $\beta_{Yield} \text{ and } \beta_{EC}$ represent the estimated change in WHC when Yield and EC are increased by 1. They are the coefficients of our model.

-   $\sigma^2$ represents the unbiased maximum likelihood for the variance.

-   $\textbf{R}$ represents the correlation matrix between observations from the data, where we have chosen an exponential correlation function.

-   ${\phi}$ represents the parameter used in the exponential correlation calculated from the data.

-   ${\omega}$ represents the variance "nugget" to capture same-location variability.

\[Can you add the model that we use and the parameters and stuff here\]

#Predictive model

#Implementation of at least four standard supervised machine learning models We tried a variety of model to get try to get better predictions and compare those results. Indeed, we fit a Random Forest model, Decision Tree, Gradient boosting and KNN model and already mentioned Poisson model.

#Significant or innovative feature engineering

Since we noticed that that time was an important factor, we considered to add generate more variables that can handle the seasonality presented in the data.

```{python,echo=TRUE, eval=FALSE}
data['Month_Sin'] = np.sin(2 * np.pi * data['Month']/12)
data['Month_Cos'] = np.cos(2 * np.pi * data['Month']/12)
data['Day_Sin'] = np.sin(2 * np.pi * data['Day']/data['Date'].dt.days_in_month)
data['Day_Cos'] = np.cos(2 * np.pi * data['Day']/data['Date'].dt.days_in_month)
```

This feature engineering lies in capturing temporal patterns in a way that is more suitable for machine learning models, like this Random Forest. By encoding months and days as cyclical features, the model can understand their cyclic nature and learn patterns effectively, such as monthly or daily seasonality which is present in the crime data because of weather and another factors.

#An exploration of variable importance using SHAP

As we mentioned before, we used a Decision Tree to understand better the factors that impact the count of violent crime.

```{python, echo=TRUE, eval=FALSE}
rf_model = RandomForestRegressor(n_estimators=20, random_state=42)

# Fit the model
rf_model.fit(X, y)

# Get feature importances
feature_importances = rf_model.feature_importances_

# Map feature names to their respective importances
feature_importance_map = dict(zip(X.columns, feature_importances))

# Sort the features by importance
sorted_features = sorted(feature_importance_map.items(), key=lambda x: x[1], reverse=True)

# Extract top 25 features
top_features = sorted_features[:25]
```

![Image Description](output.png) We kind of expecting that that temperature will be an important factor, so we will check with SHAP as well to understand the impact of features on model predictions.

```{python, echo=TRUE, eval=FALSE}
#Fitting the model
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

#Getting SHAP values and plotting them
explainer = shap.Explainer(rf, X_train)
shap_values = explainer.shap_values(X_test[:100])
shap.summary_plot(shap_values, features=X_test[:100], feature_names=X_train.columns)
```

![Image Description](output2.png) In summary, thanks to the SHAP values analysis, it seems that the model seems to be influenced significantly by perceived temperature (feelslike) and actual temperature measures, with time-related features also playing a key role. Lower on the importance scale are some other weather-related features and atmospheric conditions.

#Cluster analysis OR anomaly detection Also we wanted to apply cluster analysis to understand patterns or groups in how these factors correlate with crime rates

```{python, echo=TRUE, eval=FALSE}
#Starting clustering analysis
kmeans = KMeans(n_clusters=3, n_init='auto')
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

hc = AgglomerativeClustering(distance_threshold=None, n_clusters=3)
hc.fit(X)
linkage_methods = ['single', 'complete', 'average', 'ward']

# Plotting
plt.figure(figsize=(12, 8))
for i, method in enumerate(linkage_methods, 1):
    plt.subplot(2, 2, i)
    Z = linkage(X, method=method)
    dendrogram(Z)
    plt.title(f'Dendrogram ({method.capitalize()} Linkage)')
    plt.xlabel('Samples')
    plt.ylabel('Distance')

plt.tight_layout()
plt.show()
```

![Image Description](output3.png) It seems that they're 3 main clusters in the data. This is confirm with the "elbow method" technique, since around 3 or 4 it seems to be the place where the rate of decrease in WCSS slows down significantly.

```{python, echo=TRUE, eval=FALSE}
# Calculating WCSS by K number of clusters for plot
kmeans_per_k = [KMeans(n_clusters=k, n_init='auto', random_state=42).fit(X)
                for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]


#Plotting WCSS by number of clusters
plt.figure(figsize=(10,4))
plt.plot(np.arange(len(inertias))+1,inertias,marker="o")
plt.xlabel('Number of Clusters, K')
plt.ylabel('WCSS')
```

![Image Description](output4.png) After defining the number of clusters, we can start analyzing the major factors with these 3 clusters.

```{python, echo=TRUE, eval=FALSE}
#Selecting factors to see
plot_df = pd.DataFrame(X[['feelslike', 'Year', 'YrMon', 'Date', 'tempmax', 'IsWeekend', 'feelslikemax', 'temp', 'DayOfYear', 'WeekOfYear']])

#Plotting
plot_df['clusters'] = y_kmeansf Clusters, K')
plt.ylabel('WCSS')
sns.pairplot(plot_df, hue='clusters', palette='Dark2')
```

![Image Description](output5.png) From the clustering analysis, we can learn that the feature relationships, distribution, and seasonality.
